import SwiftUI
import Vision
import CoreImage
import CoreImage.CIFilterBuiltins

// MARK: - Image Segmentation Service (Person Background Removal)
/// Service qui utilise Vision Framework pour d√©tourer automatiquement les personnes
/// et cr√©er des images avec fond transparent pour l'effet glass bubble
@MainActor
final class ImageSegmentationService: ObservableObject {
    static let shared = ImageSegmentationService()

    // Cache des images segment√©es (√©vite de retraiter √† chaque affichage)
    private var segmentedImagesCache: [String: UIImage] = [:]

    // Context Core Image pour le traitement
    private let ciContext: CIContext

    private init() {
        // Cr√©er un contexte optimis√© pour le traitement d'images
        self.ciContext = CIContext(options: [
            .useSoftwareRenderer: false,
            .highQualityDownsample: true
        ])
    }

    // MARK: - Public API

    /// Retourne une image d√©tour√©e (fond transparent) √† partir du nom d'asset
    /// - Parameter imageName: Nom de l'image dans les Assets
    /// - Returns: UIImage avec fond transparent, ou l'originale si √©chec
    func getSegmentedImage(named imageName: String) async -> UIImage? {
        // V√©rifier le cache d'abord
        if let cached = segmentedImagesCache[imageName] {
            print("‚úÖ Cache hit for \(imageName)")
            return cached
        }

        // Charger l'image originale
        guard let originalImage = UIImage(named: imageName) else {
            print("‚ùå Image not found: \(imageName)")
            return nil
        }

        print("üîÑ Processing segmentation for \(imageName)...")

        // Effectuer la segmentation
        if let segmented = await segmentPerson(from: originalImage) {
            segmentedImagesCache[imageName] = segmented
            print("‚úÖ Segmentation complete for \(imageName)")
            return segmented
        }

        print("‚ö†Ô∏è Segmentation failed for \(imageName), returning original")
        // Fallback: retourner l'originale
        return originalImage
    }

    /// Pr√©charge et segmente toutes les images de profil
    func preloadAllProfileImages() async {
        let profileNames = [
            "photo_gil", "photo_denis", "photo_shay",
            "photo_salome", "photo_dan", "photo_gilles", "photo_judith"
        ]

        for name in profileNames {
            _ = await getSegmentedImage(named: name)
        }
    }

    // MARK: - Person Segmentation (Vision Framework)

    /// Segmente une personne de l'arri√®re-plan en utilisant Vision
    private func segmentPerson(from image: UIImage) async -> UIImage? {
        guard let cgImage = image.cgImage else {
            print("‚ùå Cannot get CGImage")
            return nil
        }

        print("üîÑ Starting Vision segmentation for image \(cgImage.width)x\(cgImage.height)...")

        // Ex√©cuter Vision sur un thread background
        let maskBuffer: CVPixelBuffer? = await withCheckedContinuation { continuation in
            DispatchQueue.global(qos: .userInitiated).async {

                // Essayer d'abord VNGeneratePersonSegmentationRequest
                let personRequest = VNGeneratePersonSegmentationRequest()
                personRequest.qualityLevel = .accurate
                personRequest.outputPixelFormat = kCVPixelFormatType_OneComponent8

                let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

                do {
                    try handler.perform([personRequest])

                    if let result = personRequest.results?.first {
                        print("‚úÖ VNGeneratePersonSegmentationRequest succeeded!")
                        continuation.resume(returning: result.pixelBuffer)
                        return
                    } else {
                        print("‚ö†Ô∏è VNGeneratePersonSegmentationRequest returned no results")
                    }
                } catch {
                    print("‚ö†Ô∏è VNGeneratePersonSegmentationRequest failed: \(error.localizedDescription)")
                }

                // Fallback: essayer VNGenerateForegroundInstanceMaskRequest (iOS 17+)
                if #available(iOS 17.0, *) {
                    print("üîÑ Trying VNGenerateForegroundInstanceMaskRequest fallback...")
                    let foregroundRequest = VNGenerateForegroundInstanceMaskRequest()

                    do {
                        try handler.perform([foregroundRequest])

                        if let result = foregroundRequest.results?.first {
                            // G√©n√©rer le masque pour toutes les instances
                            let allInstances = result.allInstances
                            if let maskPixelBuffer = try? result.generateScaledMaskForImage(forInstances: allInstances, from: handler) {
                                print("‚úÖ VNGenerateForegroundInstanceMaskRequest succeeded!")
                                continuation.resume(returning: maskPixelBuffer)
                                return
                            }
                        }
                        print("‚ö†Ô∏è VNGenerateForegroundInstanceMaskRequest returned no usable mask")
                    } catch {
                        print("‚ö†Ô∏è VNGenerateForegroundInstanceMaskRequest failed: \(error.localizedDescription)")
                    }
                }

                print("‚ùå All segmentation methods failed")
                continuation.resume(returning: nil)
            }
        }

        // Appliquer le masque sur le main thread (o√π ciContext vit)
        guard let maskBuffer = maskBuffer else {
            print("‚ùå No mask buffer returned from Vision - segmentation failed")
            return nil
        }

        print("üîÑ Applying mask to create transparent image...")
        return createTransparentImage(originalCGImage: cgImage, maskBuffer: maskBuffer)
    }

    /// Cr√©e une image avec fond transparent en utilisant le masque comme canal alpha
    private func createTransparentImage(originalCGImage: CGImage, maskBuffer: CVPixelBuffer) -> UIImage? {
        print("üîç Creating transparent image...")

        // Convertir l'image originale en CIImage
        let originalCIImage = CIImage(cgImage: originalCGImage)

        // Convertir le masque CVPixelBuffer en CIImage
        let maskCIImage = CIImage(cvPixelBuffer: maskBuffer)

        print("üîç Original size: \(originalCIImage.extent)")
        print("üîç Mask size: \(maskCIImage.extent)")

        // Redimensionner le masque pour correspondre √† l'image originale
        let scaleX = originalCIImage.extent.width / maskCIImage.extent.width
        let scaleY = originalCIImage.extent.height / maskCIImage.extent.height

        var scaledMask = maskCIImage.transformed(by: CGAffineTransform(scaleX: scaleX, y: scaleY))

        // S'assurer que le masque est bien align√© avec l'image originale
        scaledMask = scaledMask.transformed(by: CGAffineTransform(translationX: originalCIImage.extent.origin.x - scaledMask.extent.origin.x,
                                                                   y: originalCIImage.extent.origin.y - scaledMask.extent.origin.y))

        print("üîç Scaled mask size: \(scaledMask.extent)")

        // M√âTHODE 1: Utiliser le masque comme canal alpha directement
        // Composer: RGB de l'original + Alpha du masque

        // D'abord, convertir le masque grayscale en alpha
        guard let maskToAlphaFilter = CIFilter(name: "CIMaskToAlpha") else {
            print("‚ùå Cannot create CIMaskToAlpha filter")
            // Fallback: essayer avec CIBlendWithAlphaMask
            return createTransparentImageWithBlend(originalCIImage: originalCIImage, maskCIImage: scaledMask)
        }

        maskToAlphaFilter.setValue(scaledMask, forKey: kCIInputImageKey)

        guard let alphaMask = maskToAlphaFilter.outputImage else {
            print("‚ùå CIMaskToAlpha produced no output, trying blend approach")
            return createTransparentImageWithBlend(originalCIImage: originalCIImage, maskCIImage: scaledMask)
        }

        // Utiliser CIBlendWithAlphaMask pour combiner
        guard let blendFilter = CIFilter(name: "CIBlendWithAlphaMask") else {
            print("‚ùå Cannot create CIBlendWithAlphaMask filter")
            return createTransparentImageWithBlend(originalCIImage: originalCIImage, maskCIImage: scaledMask)
        }

        // Background transparent
        let clearImage = CIImage(color: CIColor(red: 0, green: 0, blue: 0, alpha: 0))
            .cropped(to: originalCIImage.extent)

        blendFilter.setValue(originalCIImage, forKey: kCIInputImageKey)
        blendFilter.setValue(clearImage, forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(alphaMask, forKey: kCIInputMaskImageKey)

        guard let outputCIImage = blendFilter.outputImage else {
            print("‚ùå CIBlendWithAlphaMask produced no output")
            return createTransparentImageWithBlend(originalCIImage: originalCIImage, maskCIImage: scaledMask)
        }

        return renderToUIImage(outputCIImage, extent: originalCIImage.extent)
    }

    /// M√©thode alternative avec CIBlendWithMask
    private func createTransparentImageWithBlend(originalCIImage: CIImage, maskCIImage: CIImage) -> UIImage? {
        print("üîç Trying CIBlendWithMask fallback...")

        guard let blendFilter = CIFilter(name: "CIBlendWithMask") else {
            print("‚ùå Cannot create CIBlendWithMask filter")
            return nil
        }

        let clearImage = CIImage(color: CIColor(red: 0, green: 0, blue: 0, alpha: 0))
            .cropped(to: originalCIImage.extent)

        blendFilter.setValue(originalCIImage, forKey: kCIInputImageKey)
        blendFilter.setValue(clearImage, forKey: kCIInputBackgroundImageKey)
        blendFilter.setValue(maskCIImage, forKey: kCIInputMaskImageKey)

        guard let outputCIImage = blendFilter.outputImage else {
            print("‚ùå CIBlendWithMask produced no output")
            return nil
        }

        return renderToUIImage(outputCIImage, extent: originalCIImage.extent)
    }

    /// Render CIImage to UIImage with transparency support
    private func renderToUIImage(_ ciImage: CIImage, extent: CGRect) -> UIImage? {
        // Utiliser un format qui supporte la transparence
        let format = CIFormat.RGBA8
        let colorSpace = CGColorSpaceCreateDeviceRGB()

        guard let outputCGImage = ciContext.createCGImage(
            ciImage,
            from: extent,
            format: format,
            colorSpace: colorSpace
        ) else {
            print("‚ùå Cannot render final CGImage")
            return nil
        }

        print("‚úÖ Created transparent image! Size: \(outputCGImage.width)x\(outputCGImage.height), hasAlpha: \(outputCGImage.alphaInfo != .none)")
        return UIImage(cgImage: outputCGImage)
    }

    // MARK: - Cache Management

    /// Vide le cache des images segment√©es
    func clearCache() {
        segmentedImagesCache.removeAll()
    }

    /// V√©rifie si une image est d√©j√† en cache
    func isCached(_ imageName: String) -> Bool {
        return segmentedImagesCache[imageName] != nil
    }
}

// MARK: - SwiftUI Image Extension pour faciliter l'usage
extension Image {
    /// Cr√©e une Image √† partir d'une UIImage optionnelle avec fallback
    init(uiImage: UIImage?, fallbackSystemName: String) {
        if let uiImage = uiImage {
            self.init(uiImage: uiImage)
        } else {
            self.init(systemName: fallbackSystemName)
        }
    }
}

// MARK: - Async Image View avec Segmentation
/// Vue qui charge et affiche une image segment√©e de mani√®re asynchrone
struct SegmentedAsyncImage: View {
    let imageName: String
    let size: CGSize
    let placeholderColor: Color

    @State private var segmentedImage: UIImage?
    @State private var isLoading = true

    var body: some View {
        ZStack {
            if let image = segmentedImage {
                Image(uiImage: image)
                    .resizable()
                    .aspectRatio(contentMode: .fill)
                    .frame(width: size.width, height: size.height)
                    .clipShape(Circle())
            } else if isLoading {
                // Placeholder pendant le chargement avec spinner
                Circle()
                    .fill(
                        RadialGradient(
                            colors: [
                                placeholderColor.opacity(0.15),
                                placeholderColor.opacity(0.25)
                            ],
                            center: .center,
                            startRadius: 0,
                            endRadius: size.width * 0.5
                        )
                    )
                    .frame(width: size.width, height: size.height)
                    .overlay(
                        ProgressView()
                            .scaleEffect(0.8)
                            .tint(placeholderColor.opacity(0.6))
                    )
            } else {
                // Fallback si pas d'image - ic√¥ne personne
                Image(systemName: "person.fill")
                    .font(.system(size: size.width * 0.35, weight: .medium))
                    .foregroundStyle(
                        LinearGradient(
                            colors: [
                                placeholderColor.opacity(0.8),
                                placeholderColor.opacity(0.6)
                            ],
                            startPoint: .top,
                            endPoint: .bottom
                        )
                    )
            }
        }
        .task {
            await loadSegmentedImage()
        }
    }

    private func loadSegmentedImage() async {
        isLoading = true
        segmentedImage = await ImageSegmentationService.shared.getSegmentedImage(named: imageName)
        isLoading = false
    }
}
